{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPpRj0WOHK1lDsM2rkQvDHZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tdh424/DL-MR-GREEN/blob/main/cot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chain-of-Thought Reasoning (CoT)"
      ],
      "metadata": {
        "id": "gJV1r8XQhDeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import re\n",
        "import torch\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration #from huggingface\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "mjjQYYwdhGVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 - Downloading model from HuggingFace"
      ],
      "metadata": {
        "id": "4TCmm0DzjNGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-base\"\n"
      ],
      "metadata": {
        "id": "WVDTasm5k-GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbmodVMPWhvI"
      },
      "outputs": [],
      "source": [
        "#load the tokenizer from pretrained model\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "# Load model\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name) #the transformer model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 - Loading and formatting"
      ],
      "metadata": {
        "id": "_ywEL6iu9GN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"tasksource/ScienceQA_text_only\", split=\"train\")\n"
      ],
      "metadata": {
        "id": "gqPxuskCvCM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#only include samples with 4 choices form the dataset\n",
        "dataset_4 = dataset.filter(lambda x: len(x[\"choices\"]) == 4)\n",
        "#print(dataset_4)\n",
        "\n",
        "\n",
        "random.seed(42)  #it is the answer to life, the universe and everything XD\n",
        "sampled = random.sample(list(dataset_4), 200)\n",
        "\n",
        "\n",
        "def format_question(example):\n",
        "    question = example[\"question\"]\n",
        "    choices = example[\"choices\"]\n",
        "\n",
        "    return (\n",
        "        f\"Question: {question}\\n\"\n",
        "        f\"Options:\\n\"\n",
        "        f\"(A) {choices[0]}\\n\"\n",
        "        f\"(B) {choices[1]}\\n\"\n",
        "        f\"(C) {choices[2]}\\n\"\n",
        "        f\"(D) {choices[3]}\\n\"\n",
        "    )\n",
        "\n",
        "formatted_questions = [format_question(q) for q in sampled] #for all 200 samples\n",
        "formatted_questions[0:5] #vizualize first 5 formatted questions"
      ],
      "metadata": {
        "id": "D9_OEJv6XPB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 - The baseline"
      ],
      "metadata": {
        "id": "6hTRw-_U9Hxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#the structure of the baseline prompt\n",
        "def make_baseline_prompt(formatted_q: str) -> str:\n",
        "    return formatted_q + \"Answer:\"\n",
        "\n",
        "baseline_prompts = [make_baseline_prompt(q) for q in formatted_questions]\n",
        "print(baseline_prompts[0])\n",
        "\n",
        "\n",
        "\n",
        "#predict function\n",
        "LETTER_RE = re.compile(r\"\\b([ABCD])\\b\")\n",
        "\n",
        "def predict_letter(prompt: str, model, tokenizer, max_new_tokens: int = 5) -> str:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "    text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    m = LETTER_RE.search(text)\n",
        "    return m.group(1) if m else \"INVALID\" #if no letter found then return INVALID\n",
        "\n",
        "\n",
        "#run baseline on all questions\n",
        "model.eval()\n",
        "torch.set_num_threads(1)\n",
        "\n",
        "baseline_predictions = []\n",
        "\n",
        "for prompt in tqdm(baseline_prompts, desc=\"Baseline (no CoT)\"):\n",
        "    pred = predict_letter(prompt, model, tokenizer)\n",
        "    baseline_predictions.append(pred)\n",
        "\n",
        "#show first 5 results\n",
        "for i in range(5):\n",
        "    print(f\"Q{i}: {baseline_predictions[i]}\")\n"
      ],
      "metadata": {
        "id": "bFgc7XCV9INS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 - Use of CoT"
      ],
      "metadata": {
        "id": "IeqV91eW9DQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#our CoT instruction\n",
        "COT_INSTR = (\n",
        "    \"You are a careful reasoning assistant.\\n\"\n",
        "    \"Think through the problem step by step.\\n\"\n",
        "    \"Write three short reasoning steps.\\n\"\n",
        "    \"Then, output the final answer as a single letter (A–D) on its own line.\\n\\n\"\n",
        ")\n",
        "\n",
        "def make_cot_prompt(formatted_q: str) -> str:\n",
        "    return COT_INSTR + formatted_q.strip() + \"\\n\"\n",
        "\n",
        "#parsing\n",
        "LETTER_RE = re.compile(r\"\\b([ABCD])\\b\")\n",
        "\n",
        "torch.set_num_threads(1)\n",
        "device = torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_text(prompt: str, max_new_tokens: int = 120) -> str:\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=False,          #greedy decoding\n",
        "        max_new_tokens=max_new_tokens,\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True).strip()\n",
        "\n",
        "def parse_output(text: str):\n",
        "    lines = [l.strip() for l in text.split(\"\\n\") if l.strip()]\n",
        "\n",
        "    # reasoning = everything except last line\n",
        "    reasoning = lines[:-1][:3]\n",
        "    while len(reasoning) < 3:\n",
        "        reasoning.append(\"\")\n",
        "\n",
        "    #final answer = last detectable letter\n",
        "    m = LETTER_RE.findall(text)\n",
        "    answer = m[-1] if m else \"INVALID\"\n",
        "\n",
        "    return reasoning[0], reasoning[1], reasoning[2], answer\n",
        "\n",
        "#run CoT\n",
        "rows = []\n",
        "invalid_count = 0\n",
        "\n",
        "for q in tqdm(formatted_questions, desc=\"Running CoT prompts (T5-base)\"):\n",
        "    prompt = make_cot_prompt(q)\n",
        "    out_text = generate_text(prompt)\n",
        "\n",
        "    r1, r2, r3, ans = parse_output(out_text)\n",
        "    if ans == \"INVALID\":\n",
        "        invalid_count += 1\n",
        "\n",
        "    rows.append({\n",
        "        \"prompt\": prompt,\n",
        "        \"output\": out_text,\n",
        "        \"reason1\": r1,\n",
        "        \"reason2\": r2,\n",
        "        \"reason3\": r3,\n",
        "        \"pred_cot\": ans\n",
        "    })\n",
        "\n",
        "cot_df = pd.DataFrame(rows)\n",
        "\n",
        "#visualization\n",
        "print(f\"INVALID rate: {invalid_count / len(cot_df) * 100:.1f}%\")\n",
        "print(\"First 5 predicted letters:\", cot_df[\"pred_cot\"].head(5).tolist())\n",
        "print(\"\\nExample output (first item):\")\n",
        "print(cot_df.loc[0, \"output\"])\n"
      ],
      "metadata": {
        "id": "aSXIUCVEWjs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 - Flip analysis"
      ],
      "metadata": {
        "id": "goD8PRm35l_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#building gold answers (A–D)\n",
        "IDX2LETTER = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
        "gold_answers = [IDX2LETTER[int(ex[\"answer\"])] for ex in sampled]  # length 200\n",
        "\n",
        "\n",
        "baseline_preds = baseline_predictions                      # from Task 4.3\n",
        "cot_preds = cot_df[\"pred_cot\"].tolist()                    # from Task 4.4\n",
        "\n",
        "assert len(gold_answers) == len(baseline_preds) == len(cot_preds), \"Lengths don't match.\"\n",
        "\n",
        "#INVALID must be treated as incorrect automatically\n",
        "def is_correct(pred, gold):\n",
        "    return pred in [\"A\",\"B\",\"C\",\"D\"] and pred == gold\n",
        "\n",
        "#categorizing the flips\n",
        "categories = []\n",
        "for g, b, c in zip(gold_answers, baseline_preds, cot_preds):\n",
        "    b_ok = is_correct(b, g)\n",
        "    c_ok = is_correct(c, g)\n",
        "\n",
        "    if (not b_ok) and c_ok:\n",
        "        categories.append(\"Flip-to-correct\")\n",
        "    elif b_ok and (not c_ok):\n",
        "        categories.append(\"Flip-to-incorrect\")\n",
        "    elif b_ok and c_ok:\n",
        "        categories.append(\"Correct–Correct\")\n",
        "    else:\n",
        "        categories.append(\"Incorrect–Incorrect\")\n",
        "\n",
        "counts = Counter(categories)\n",
        "\n",
        "#accuracies\n",
        "baseline_acc = np.mean([is_correct(b, g) for g, b in zip(gold_answers, baseline_preds)])\n",
        "cot_acc = np.mean([is_correct(c, g) for g, c in zip(gold_answers, cot_preds)])\n",
        "\n",
        "print(\"Counts by category:\")\n",
        "for k in [\"Flip-to-correct\", \"Flip-to-incorrect\", \"Correct–Correct\", \"Incorrect–Incorrect\"]:\n",
        "    print(f\"  {k}: {counts.get(k, 0)}\")\n",
        "\n",
        "print(f\"\\nAccuracy (no CoT):  {baseline_acc*100:.1f}%\")\n",
        "print(f\"Accuracy (with CoT): {cot_acc*100:.1f}%\")\n",
        "\n",
        "#the Bar plot\n",
        "labels = [\"Flip-to-correct\", \"Flip-to-incorrect\", \"Correct–Correct\", \"Incorrect–Incorrect\"]\n",
        "values = [counts.get(k, 0) for k in labels]\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(labels, values)\n",
        "plt.xticks(rotation=25, ha=\"right\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.title(\"Flip analysis: baseline vs. CoT\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "flip_df = pd.DataFrame({\n",
        "    \"gold\": gold_answers,\n",
        "    \"pred_baseline\": baseline_preds,\n",
        "    \"pred_cot\": cot_preds,\n",
        "    \"category\": categories,\n",
        "    \"cot_reason1\": cot_df[\"reason1\"],\n",
        "    \"cot_reason2\": cot_df[\"reason2\"],\n",
        "    \"cot_reason3\": cot_df[\"reason3\"],\n",
        "    \"cot_output\": cot_df[\"output\"],\n",
        "})\n",
        "flip_df.head()\n"
      ],
      "metadata": {
        "id": "z2z5s3WP5oTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.6 - Qualitative inspection"
      ],
      "metadata": {
        "id": "wEVShr_ePu16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Just to be more clear\n",
        "categories = labels\n",
        "\n",
        "def sample_and_print(df, category, n=20, seed=42):\n",
        "    subset = df[df[\"category\"] == category]\n",
        "    k = min(n, len(subset))\n",
        "\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(f\"{category}  |  showing {k} of {len(subset)}\")\n",
        "    print(\"=\"*90)\n",
        "\n",
        "    if k == 0:\n",
        "        print(\"(No examples in this category.)\")\n",
        "        return subset\n",
        "\n",
        "    sampled = subset.sample(n=k, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "    for i, row in sampled.iterrows():\n",
        "        print(\"\\n\" + \"-\"*90)\n",
        "        print(f\"[{i+1}/{k}]  gold={row['gold']}  \"\n",
        "              f\"baseline={row['pred_baseline']}  CoT={row['pred_cot']}\")\n",
        "\n",
        "        print(\"\\nCoT reasoning steps:\")\n",
        "        print(f\"1) {row['cot_reason1']}\")\n",
        "        print(f\"2) {row['cot_reason2']}\")\n",
        "        print(f\"3) {row['cot_reason3']}\")\n",
        "\n",
        "        print(\"\\nRAW CoT OUTPUT:\")\n",
        "        print(row[\"cot_output\"])\n",
        "\n",
        "    return sampled\n",
        "\n",
        "\n",
        "samples = {}\n",
        "for cat in categories:\n",
        "    samples[cat] = sample_and_print(flip_df, cat, n=20, seed=42)\n"
      ],
      "metadata": {
        "id": "DFvSZEKoBQJ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}